{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06910dd",
   "metadata": {
    "id": "e06910dd"
   },
   "source": [
    "# Построение пайплайна в Airflow\n",
    "\n",
    "- Автор: Онищенко Надежда\n",
    "- Дата: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149134c",
   "metadata": {
    "id": "4149134c"
   },
   "source": [
    "## Цели и задачи проекта\n",
    "\n",
    "Сервис онлайн-книги предоставляет доступ к контенту разных форматов, включая текст, аудио и не только. Построить пайплайн в Airflow, который будет запускать PySpark-скрипт для обработки данных и создания витрин. Эти витрины помогут команде сервиса быстрее и проще готовить отчёты.\n",
    "\n",
    "Создать DAG для запуска Spark-кода."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728986ea",
   "metadata": {
    "id": "728986ea"
   },
   "source": [
    "## Описание данных\n",
    "\n",
    "Таблица `bookmate.audition` содержит данные об активности пользователей и включает столбцы:\n",
    "\n",
    "* `audition_id` — уникальный идентификатор сессии чтения или прослушивания;\n",
    "\n",
    "* `puid` — идентификатор пользователя;\n",
    "\n",
    "* `usage_platform_ru` — название платформы, с помощью которой пользователь взаимодействует с контентом;\n",
    "\n",
    "* `msk_business_dt_str` — дата и время события (строка, часовой пояс — МСК);\n",
    "\n",
    "* `app_version` — версия приложения;\n",
    "\n",
    "* `adult_content_flg` — значение, которое показывает, был ли контент для взрослых (`True` или `False`);\n",
    "\n",
    "* `hours` — длительность сессии чтения или прослушивания в часах;\n",
    "\n",
    "* `hours_sessions_long` — длительность длинных сессий в часах;\n",
    "\n",
    "* `kids_content_flg` — значение, которое показывает, был ли это детский контент (`True` или `False`);\n",
    "\n",
    "* `main_content_id` — идентификатор основного контента;\n",
    "\n",
    "* `usage_geo_id` — идентификатор географического местоположения пользователя.\n",
    "\n",
    "Таблица `bookmate.content` включает столбцы:\n",
    "\n",
    "* `main_content_id` — идентификатор основного контента;\n",
    "\n",
    "* `main_author_id` — идентификатор основного автора контента;\n",
    "\n",
    "* `main_content_type` — тип контента: аудио, текст или другой;\n",
    "\n",
    "* `main_content_name` — название контента;\n",
    "\n",
    "* `main_content_duration_hours` — длительность контента в часах;\n",
    "\n",
    "* `published_topic_title_list` — список жанров или тем контента.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecaf95",
   "metadata": {
    "id": "4fecaf95"
   },
   "source": [
    "## Содержимое проекта\n",
    "\n",
    "Проект предполагает несколько шагов:\n",
    "\n",
    "1. Написание Spark-код — подключение к хранилищу данных и указание, куда сохранять результат.\n",
    "\n",
    "2. Создание DAG — запуск Spark-кода.\n",
    "\n",
    "3. Запуск Airflow — управление созданным пайплайном."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WGKIjeRdzV0U",
   "metadata": {
    "id": "WGKIjeRdzV0U"
   },
   "source": [
    "### Написание Spark-кода"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QjNj1KvlzXu2",
   "metadata": {
    "id": "QjNj1KvlzXu2"
   },
   "source": [
    "Данные для подключения к DBeaver:\n",
    "\n",
    "*   Имя пользователя — \"da_***\"\n",
    "*   Пароль — \"3***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ba945-edda-4404-a20a-75a19aa86ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clickhouse_connect import get_client\n",
    "\n",
    "client = get_client(\n",
    "    host=\"***.cloud.net\", # порт и параметры кластера ClickHouse\n",
    "    port=****,\n",
    "    username=\"da_***\", # логин\n",
    "    password=\"3***\",  # пароль \n",
    "    secure=True,\n",
    "    verify=False,\n",
    "    database=\"ground_***\"  # база\n",
    ")\n",
    "\n",
    "print(client.query(\"SHOW TABLES\").result_rows)\n",
    "print(client.query(\"SELECT * FROM bookmate_user_aggregate LIMIT 10\").result_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d21d6",
   "metadata": {
    "id": "6f7d21d6"
   },
   "source": [
    "В коде ниже приведён написанный Spark-скрипт. Ваша задача — правильно указать данные для подключения к вашему хранилищу: порты, параметры кластера ClickHouse и путь, куда будут записываться агрегаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a1b76-eccf-48b3-a9f4-a88ff1399135",
   "metadata": {
    "id": "05faf34b"
   },
   "outputs": [],
   "source": [
    "# filename=my_spark_job.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import sys\n",
    "\n",
    "# Создаём Spark-сессию и при необходимости добавляем конфигурации\n",
    "spark = SparkSession.builder.appName(\"myAggregateTest\").config(\"fs.s3a.endpoint\", \"***cloud.net\").getOrCreate()\n",
    "\n",
    "# Указываем порт и параметры кластера ClickHouse\n",
    "jdbcPort = ****\n",
    "jdbcHostname = \"***cloud.net\"\n",
    "jdbcDatabase = \"ground_***\"\n",
    "jdbcUrl = f\"jdbc:clickhouse://{jdbcHostname}:{jdbcPort}/{jdbcDatabase}?ssl=true\" # путь для записи агрегата\n",
    "\n",
    "# Получаем аргумент из Airflow\n",
    "my_date = sys.argv[1].replace('-', '_')\n",
    "\n",
    "# Считываем исходные данные за нужную дату\n",
    "df = spark.read.csv(f\"s3a://da-plus-dags/script_*/data_{my_date}/*.csv\", inferSchema=True, header=True)\n",
    "\n",
    "# Строим агрегат по пользователям\n",
    "result_df = df.groupBy(\"puid\").agg(\n",
    "    F.countDistinct(\"audition_id\").alias(\"audition_count\"),\n",
    "    F.avg(\"hours\").alias(\"avg_hours\")\n",
    ")\n",
    "\n",
    "result_df.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbcUrl) \\\n",
    "    .option(\"user\", \"da_***\") \\\n",
    "    .option(\"password\", \"3***\") \\\n",
    "    .option(\"dbtable\", \"bookmate_user_aggregate\") \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6-FZswEsB2P",
   "metadata": {
    "id": "p6-FZswEsB2P"
   },
   "source": [
    "- В результате будет создан файл с названием, указанным в первой строке. Этот файл можно будет запустить с помощью Airflow, но сначала настроим DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e82e2",
   "metadata": {
    "id": "956e82e2"
   },
   "source": [
    "### Создание DAG\n",
    "\n",
    "Теперь, когда Spark-код готов, создим DAG, который будет его запускать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vMNkNpPYsKS-",
   "metadata": {
    "id": "vMNkNpPYsKS-"
   },
   "source": [
    "#### Создание «каркаса» нового DAG. \n",
    "\n",
    "- DAG должен запускаться каждый день начиная с 1 января 2025 года. При этом запускать DAG за пропущенные даты не нужно.\n",
    "\n",
    "- Используйем менеджер контекста `with ... as dag:` — так все задачи будут корректно привязаны к DAG. После конструкции пока напишим `pass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa14ebb-b304-4081-ac97-76076ff3c6d5",
   "metadata": {
    "id": "67956690"
   },
   "outputs": [],
   "source": [
    "# filename=my_spark_job.py\n",
    "\n",
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "from airflow.sensors.s3_key_sensor import S3KeySensor\n",
    "from airflow.providers.yandex.operators.dataproc import DataprocCreatePysparkJobOperator\n",
    "\n",
    "class PysparkJobOperator(DataprocCreatePysparkJobOperator): # создаем оператор\n",
    "    template_fields = (\"cluster_id\", \"args\",)\n",
    "\n",
    "DAG_ID = \"audition_content_analysis\"\n",
    "\n",
    "with DAG(\n",
    "    dag_id=DAG_ID,\n",
    "    schedule_interval=\"@daily\", # ежедневно\n",
    "    start_date=datetime(2025, 1, 1), # с 1 января 2025 года\n",
    "    catchup=False\n",
    ") as dag:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MtrlsIUGsVX1",
   "metadata": {
    "id": "MtrlsIUGsVX1"
   },
   "source": [
    "#### Добавление проверки входного файла. \n",
    "\n",
    "- DAG не должен стартовать, пока в S3 не появится файл с данными за нужную дату.\n",
    "\n",
    "- Для решения используем сенсор `S3KeySensor`. Он должен проверять наличие файла каждые 5 минут и ждать максимум час. В качестве аргумента для параметра `bucket_name` укажем строку `\"da-plus-dags\"`.\n",
    "\n",
    "- Файл называется `*.csv`, добавим дату запуска в формате `YYYY_MM_DD`. Путь к данным  должен быть аргументом для параметра `bucket_key` в `S3KeySensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877e3ebb-200a-41fc-a0e9-3ae2f23730f0",
   "metadata": {
    "id": "b55c46cb"
   },
   "outputs": [],
   "source": [
    "# filename=my_spark_job.py\n",
    "\n",
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "from airflow.sensors.s3_key_sensor import S3KeySensor\n",
    "from airflow.providers.yandex.operators.dataproc import DataprocCreatePysparkJobOperator\n",
    "\n",
    "class PysparkJobOperator(DataprocCreatePysparkJobOperator):\n",
    "    template_fields = (\"cluster_id\", \"args\",)\n",
    "\n",
    "DAG_ID = \"audition_content_analysis\"\n",
    "\n",
    "wait_for_input = S3KeySensor(\n",
    "    task_id=\"wait_for_input\",\n",
    "    bucket_name=\"da-plus-dags\",                                   # Имя бакета\n",
    "    bucket_key=\"script_*/data_{{ ds.replace('-', '_') }}/*.csv\",  # Шаблон даты в имени файла\n",
    "    aws_conn_id=\"s3\",                                             # Подключение S3\n",
    "    poke_interval=300,                                            # Проверяем каждые 5 минут\n",
    "    timeout=3600,                                                 # Ждём максимум час\n",
    "    mode=\"poke\",                                                  # Проверка происходит периодически\n",
    "    wildcard_match=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jY8aAW9Gsb_U",
   "metadata": {
    "id": "jY8aAW9Gsb_U"
   },
   "source": [
    "#### Создание задачи для запуска Spark-скрипта через Airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d2b62-d5f2-49d9-bd89-521640776168",
   "metadata": {
    "id": "SE_morl2seZq"
   },
   "outputs": [],
   "source": [
    "# filename=my_spark_job.py\n",
    "\n",
    "from airflow.providers.yandex.operators.dataproc import DataprocCreatePysparkJobOperator\n",
    "\n",
    "user = \"da_***\"\n",
    "\n",
    "run_pyspark = PysparkJobOperator(\n",
    "    task_id=\"run_pyspark\",\n",
    "    name=\"run_pyspark_job\",\n",
    "    cluster_id=\"c***\",\n",
    "    args=[\"{{ ds }}\"],\n",
    "    main_python_file_uri=f\"s3a://da-plus-dags/{user}/jobs/my_spark_job.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0zdUDAY1shG5",
   "metadata": {
    "id": "0zdUDAY1shG5"
   },
   "source": [
    "#### Соберем все фрагменты вместе:\n",
    "\n",
    "* Опишим DAG с нужными параметрами.\n",
    "\n",
    "* Добавим сенсор для ожидания входного файла.\n",
    "\n",
    "* Добавим Spark-задачу для запуска скрипта.\n",
    "\n",
    "* Настроим зависимости так, чтобы Spark-задача запускалась только после появления файла."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec7a45-ba86-48f6-8037-61b71d669b3c",
   "metadata": {
    "id": "ieU9hC2Aznes"
   },
   "source": [
    "Данные для подключения к Airflow:\n",
    "*   IP — '**.***.***.***'\n",
    "*   Имя пользователя — 'da_***'\n",
    "*   Пароль — '3***'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b61666-fbbf-4ff4-9897-953810694162",
   "metadata": {
    "id": "uCA8ULn6sjOq"
   },
   "outputs": [],
   "source": [
    "# filename=bookmate_dag.py\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.sensors.s3_key_sensor import S3KeySensor\n",
    "from airflow.providers.yandex.operators.dataproc import DataprocCreatePysparkJobOperator\n",
    "\n",
    "class PysparkJobOperator(DataprocCreatePysparkJobOperator):\n",
    "    template_fields = (\"cluster_id\", \"args\",)\n",
    "\n",
    "DAG_ID = \"audition_content_analysis\"\n",
    "\n",
    "with DAG(\n",
    "    dag_id=DAG_ID,\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    catchup=False\n",
    ") as dag:\n",
    "    # 1) Ждём появления входного файла в S3\n",
    "    wait_for_input = S3KeySensor(\n",
    "        task_id=\"wait_for_input\",\n",
    "        bucket_name=\"da-plus-dags\",                                                         \n",
    "        bucket_key=\"script_bookmate/data_{{ ds.replace('-', '_') }}/audition_content.csv\", \n",
    "        aws_conn_id=\"s3\",                                                                  \n",
    "        poke_interval=300,                                                               \n",
    "        timeout=3600,                                                                    \n",
    "        mode=\"poke\",                                                                        \n",
    "        wildcard_match=False\n",
    "    )\n",
    "\n",
    "    # 2) Запускаем PySpark-задание на кластере Dataproc (оператор Яндекс Облака)\n",
    "    user = \"da_20250827_0e335bbca6\"\n",
    "    \n",
    "    run_pyspark = PysparkJobOperator(\n",
    "        task_id=\"run_pyspark\",\n",
    "        name=\"run_pyspark_job\",\n",
    "        cluster_id=\"c9q4134h5vi546h1e148\",\n",
    "        args=[\"{{ ds }}\"],\n",
    "        main_python_file_uri=f\"s3a://da-plus-dags/{user}/jobs/bookmate_dag.py\",\n",
    "    )\n",
    "\n",
    "    # 3)  Зависимости\n",
    "    wait_for_input >> run_pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953e84c",
   "metadata": {
    "id": "6953e84c"
   },
   "source": [
    "### Запуск Airflow\n",
    "\n",
    "Теперь можно переходить к запуску:\n",
    "- В интерфейсе найдем DAG и запустим его.\n",
    "- Проверим, что DAG выполнился, а результат соответствует ожиданиям."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 8099,
    "start_time": "2025-09-26T08:43:58.153Z"
   },
   {
    "duration": 754,
    "start_time": "2025-09-26T08:44:19.124Z"
   },
   {
    "duration": 101,
    "start_time": "2025-09-26T08:45:07.927Z"
   },
   {
    "duration": 131,
    "start_time": "2025-09-26T08:46:04.024Z"
   },
   {
    "duration": 11,
    "start_time": "2025-09-26T09:06:19.368Z"
   },
   {
    "duration": 17570,
    "start_time": "2025-09-26T09:06:25.490Z"
   },
   {
    "duration": 0,
    "start_time": "2025-09-26T09:06:43.117Z"
   },
   {
    "duration": 0,
    "start_time": "2025-09-26T09:06:43.125Z"
   },
   {
    "duration": 11,
    "start_time": "2025-09-26T09:07:02.572Z"
   },
   {
    "duration": 9,
    "start_time": "2025-09-26T09:07:11.064Z"
   },
   {
    "duration": 24,
    "start_time": "2025-09-26T09:07:16.831Z"
   },
   {
    "duration": 854,
    "start_time": "2025-09-26T09:17:22.884Z"
   },
   {
    "duration": 54,
    "start_time": "2025-09-26T12:56:57.966Z"
   },
   {
    "duration": 81,
    "start_time": "2025-09-26T14:25:18.703Z"
   },
   {
    "duration": 9,
    "start_time": "2025-09-26T14:26:28.251Z"
   },
   {
    "duration": 88,
    "start_time": "2025-09-26T14:30:23.528Z"
   },
   {
    "duration": 86,
    "start_time": "2025-09-26T14:30:51.128Z"
   }
  ],
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
